---
title: Qualitative and social research
presenters: Daniel Dohan & Katherine Rankin
layout: post
group: pastsem
date: 2019-03-11
---

This journal club served as the sequel to a previous [neuroethics seminar](https://decisionlab.ucsf.edu/pastsem/2018-11-26_social-science-neuro/){:target="\_blank"} 
about the integration of social neuroscience and the traditional social sciences, this time with a focus on research. 
Led by [Dan Dohan](https://profiles.ucsf.edu/dan.dohan){:target="\_blank"} and [Kate Rankin](https://memory.ucsf.edu/people/katherine-rankin-phd){:target="\_blank"}, 
we discussed the benefits and pitfalls of scientific research in comparison to those of qualitative research, dissecting 
methods, metrics, and models in search of biases. We called for the immersion of social scientists within scientific research 
settings, and vice versa, in order to mutually enhance their own knowledge as well as their appreciation for others' expertise. 
Throughout our conversation, we drew from arguments presented in [“Proving or Improving: On Health Care Research as a Form of 
Self-Reflection”](https://journals.sagepub.com/doi/abs/10.1177/1049732305285856){:target="\_blank"} 
(Qualitative Health Research, 2006) by Annemarie Mol and [“The Heart of the Matter. About Good Nursing and Telecare”](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2970812/){:target="\_blank"} 
(Health Care Analysis, 2010) by Jeannette Pols.

In response to Mol, who critiques clinical trials for merely addressing the biological aspects of disease without encompassing 
social elements, namely individuals’ lived experiences with illness, we posited that scientific methods and qualitative methods 
are equally flawed and equally positioned to benefit from interdisciplinary collaboration. Regarding both methodologies, we 
addressed susceptibility to potential biases, like unrepresentative sampling, relying on literature that supports one’s hypothesis, 
and evaluating methods purely based on their frequent use in prior studies. We also cautioned against designing loose methods that 
enable data tampering or deviation from the original protocol in order to produce novel results. 

We questioned what constitutes a good measure of success according to scientific investigators, who seek to understand truth through quantitative 
evidence and generalizable data, versus social scientists, who accept a probabilistic notion of truth and embrace the complexity of 
human health within broader social contexts. We acknowledged the utility of statistical significance within scientific research as a 
means of identifying studies with meaningful conclusions and determining the safety and efficacy of clinical treatments in larger 
populations. However, we also criticized the authority of p-values in the competitive journal publication process as a primary 
contributor to the p-hacking epidemic, and noted the overshadowing of social determinants of health. Where socially-driven health 
research gains insight into patient nuances and derives meaning from complex systems, it sacrifices the generalizability prioritized 
in scientific research and may not generate as wide of an impact as, for instance, a new drug from a clinical trial. 

We also discussed the tradeoff between practicality and accuracy characteristic of models used in both scientific and social science research.
Social scientists in our group shared their strategy for embedding social structures within grant applications by identifying a 
singular component, or simplified model that explains a more complicated picture, citing the [Care Ecosystem](https://memory.ucsf.edu/care-ecosystem){:target="\_blank"} 
survey intended to reveal the unmet needs of care-recipients with dementia and their caregivers as a relevant example. Those 
with neuroscience backgrounds felt similarly about their own models for explaining complex brain systems, like the basal ganglia, 
describing them as useful yet simplified tools for distilling complicated information.

We proposed that researchers borrow techniques from their disciplinary counterparts to improve the robustness of their methods, metrics, and models, 
for example by designing statistical analyses that take into account social factors, or incorporating the unique perspectives of investigators alongside those 
of patients. Dan commended the neuroscience field, in particular, for its hummility and openness to engage in the ethnographic element of working with patients.


