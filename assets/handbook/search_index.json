[
["index.html", "UCSF Decision Lab Handbook Chapter 1 Organizing Principles", " UCSF Decision Lab Handbook 2019-12-20 Chapter 1 Organizing Principles Early in medical school I received a piece of advice that I have been trying to follow since: Do what you uniquely can do. This of course has different implications for me personally than it does for you as a lab member. Let’s talk about what it means overall, then about what it means for me (since I use this thought to focus our lab’s work), and then what it means for you individually. Overall. In science there are many questions worth investigating–you can’t try to study them all or develop expertise in all the relevant methods. If you choose questions that everyone else is just as well-positioned to answer (i.e., that you’re not uniquely positioned to answer), then the only way to succeed is to work harder and be luckier than everyone else. This is not a sustainable path for a career or a life, and probably contributes to the temptations of scientific misconduct and non-reproducible practices. We all can probably contribute more to science by thinking about what questions we’re uniquely positioned to answer, whether because of our unique backgrounds/skills, temperament and interests, or special resources for which we have access that others don’t. Me and the lab. Given my interdisciplinary background in neurology and philosophy, and given our varied collaborations and the especially open culture of the MAC, I believe that our lab is uniquely positioned to address questions in neuroscience that also involve contributions, methods and insights from the humanities and social sciences. So more specifically: decision neuroscience of aging and disorders of aging, and neuroethics, and perhaps most of all questions that involve both topics together. You. If you’re developing an independent research project during your time here, I will keep asking you: does this project make effective use of methods, populations, resources and frameworks that our lab and the MAC are uniquely positioned to utilize? Your time here is a brief window during your career, and you should make use of special research and learning opportunities you have now that you may not have again. More broadly, in preparing for your career after this lab, think about where your own particular background, skills and interests are most needed and can contribute the most, and use this to evaluate what other skills you need to pick up along the way to maximize your unique contribution. In 2018 during the lab’s first big hiring wave, Ali Zahir and I tried to articulate the essentials of lab culture that we wanted to preserve and replicate in our hiring. (So you’re all here because we thought you fit the following description.) What we decided we were looking for follows from the principle above. While we think that diversity of experiences and perspectives is valuable for all science, it is particularly important to our work, and it is also essential that lab members value and respect such diversity. We’re looking for lab members with potential to make unique contributions to lab culture and our work–whether in neuroethics, clinical work with patients, neuroscience, computation, or logistics. And we are looking for people who, wherever they come from, have a track record of making the most of the learning and research opportunities available to them. In our work, we remain particularly mindful of our obligations to: Our funders. Most of the work in our lab is funded by the National Institutes of Health, and of course our infrastructure is provided by the University of California. This pays my salary, your salary, travel funds, research expenses, equipment, basically everything. This ultimately comes from taxes paid by everyone, and thus reflects a tremendous public investment in our work–work that should be creative, rigorous, and ethical. We strive to be prudent in our management of resources, but also aggressive in taking advantage of special opportunities to advance science. Research participants (and their families). Our research participants, many of whom have serious, stigmatized and/or ultimately fatal neurological disorders, expose themselves to research-related risks and inconveniences in order to contribute to knowledge that benefits society. Ultimately, unproductive (unanalyzed datasets, research products that are never shared with the broader scientific community) and non-rigorous (sloppy, poorly documented, error-prone) research is unethical, as it undermines the social purpose for which participants’ effort and time are contributed. We also honor participants’ contributions by treating them with respect, by maintaining our training in clinical competence (see below) and safety, by responding promptly to emails and other inquiries, and by safeguarding the confidentiality of research records. The scientific community (particularly our readers). We aim to contribute to a broader project of understanding the brain and the ethical issues raised by neuroscience. People who read our papers should find their own understanding enhanced by our work. So we use reliable and rigorous methods, back up our work to guard against data loss, double-check our own work and other lab members’ work, identify and acknowledge our mistakes, and carefully document what we do. When we make discoveries, we help people understand them by placing them in appropriate context, avoiding undue hype, acknowledging limitations and counterarguments, and writing clearly. We use social media, our website, press and other tools to help others (including nonspecialists) find our work and so contribute to a broader public conversation. Colleagues (both in the lab and at MAC/UCSF broadly). We strive for a lab environment in which all members can be successful. Discrimination and harassment of all forms are not tolerated–please speak with me directly if you are treated in a manner that makes you uncomfortable or hinders your ability to work and learn, or observe another lab member treated in such a way. We support each other and share our knowledge. When there are disagreements or other sources of tension, we need to be able to communicate openly about them (please again come see me for help with this). We are respectful of one another’s time and that of our collaborators and research participants, so we’re punctual and reliable about meetings and appointments. That said, please do not come in to work sick. Please promote your own health and others’ by taking time to recover, making arrangements for me and your colleagues to cover for you if necessary (they’ll be happier doing so than being exposed to whatever you’ve got, and you can return the favor later), and if you absolutely need to work doing so remotely. Ourselves. Each of us should feel that we are doing work that we’re proud of on topics that we’re excited about, that we have appropriate support, learning opportunities and mentoring to take the next steps in our careers, and that we have enough space for the people and pursuits that are important to us outside of lab. We also should acknowledge that science is inherently hard (we’re literally trying to do things no one has ever done before), and that you all are in particularly vulnerable and stressful parts of your careers. Struggling is not a sign of weakness, but is also not something you’re expected to suffer through by yourself. Please come talk to me so we can strategize together about resources, about our mutual expectations, about how your work here fits into your broader career goals, and about how lab work is structured. (I’ve had many more of these conversations with lab members than you probably realize.) Nothing we do in lab is worth doing if it doesn’t also contribute to your health and happiness. One final quick note about this handbook: at least at present, this handbook is for members of the Decision Lab, and is not currently intended for dissemination outside the lab. "],
["hours-and-expectations.html", "Chapter 2 Hours and Expectations", " Chapter 2 Hours and Expectations “Do as I say and not as I do,” but not with the usual meaning… In general, we are focused on completing the work that needs to be done, and not on observing a strict schedule. One nice thing about research is that, while many tasks such as research appointments or project meetings need to happen at specific places and times, other tasks can be more flexibly structured around personal or family obligations (dentist appointments, child or pet care, etc.) or preferences around the timing and place of work (e.g., if you prefer to work from home when you can, or are more of a morning or night person). That said, our work schedules are subject to two external constraints. The first is that research coordinators and other people in union-represented positions are subject to University-wide policy around things like work hours and overtime. The second is just that we have obligations to our funding agencies regarding the work we’ve promised to do for them, which includes that the effort you devote to those projects should map on to what we’ve allocated for your roles. At the same time, there are special research opportunities that you might want to take advantage of (e.g., for career development or other personal reasons) that are not directly tied to responsibilities on funded projects, and we wouldn’t want to unduly constrain your ability to work on those. For this reason, I make a rough distinction between people’s core responsibilities and their individual projects. Basically, “core responsibilities” are what we have allocated your funded effort to do: this is usually to work on one of our big R01-funded projects, which have pre-specified questions and aims and that will typically extend beyond any single research coordinator’s tenure. (As of 2019: DMA, GBD and Neurotech.) Opportunities for coordinators to do individual original work on these projects are limited. For mentoring, it’s often valuable (but not strictly required) for people to also have independent projects, which allow them more creative room and opportunities for learning, ideally leading to first-authored posters and hopefully manuscripts. These should usually be thematically and methodologically related to, but non-overlapping with, the questions pursued in our core funded projects. For work on core responsibilities, research coordinators should generally expect to spend no more than 40 hours a week, though with some variation week-to-week. If you are regularly spending more than 40 hours a week on these responsibilities, this means that something about your role is misconceived, and please come talk to me about it. You are not required to spend these 40 hours physically on campus, and they do not all need to be during normal working hours (if you prefer to work early or late). Now, if research reasons specifically compel you to work on a weekend, or before about 8am or after 6pm (e.g., to run a participant or set up the scanner), you can and should mark this on your timesheet for overtime. (This does not apply to cases in which you simply shift the time of your work, e.g. from morning to evening, because of a personal working time preference.) Work on your individual projects is generally not eligible for overtime. You’re also welcome to spend time during the normal working day to pursue these projects, so long as your core responsibilities are taken care of. Finally, you will notice that I will send Slack and e-mail messages on nights and weekends, and will be managing work (though at reduced efficiency) when I’m on vacation. These all reflect accommodations that I’ve made in my own working schedule that maximize the time I spend with my family, but they do not represent expectations for you. So usually, e.g., if I send you a Slack message on the weekend it’s because I’ve thought of something that I’m worried I’ll forget to tell you if I wait until Monday–unless I say otherwise, I’m not expecting a response before Monday. (Sometimes questions will come up on a paper or grant deadline that can’t actually wait, but I’ll let you know and it shouldn’t be often or routine.) And while I take my work with me on vacation, I’m expecting you to actually be off Slack and e-mail while you’re away–we have enough people in lab that we should be able to make accommodations for covering your responsibilities before you leave. "],
["lab-resources-overview.html", "Chapter 3 Lab Resources Overview 3.1 Slack 3.2 Lab website 3.3 Private Github repo 3.4 R: drive (R:) 3.5 LAVA and LAVAquery 3.6 Deprecated/discouraged tools", " Chapter 3 Lab Resources Overview Here is a very general overview of lab-wide resources that everyone in both sides of the lab should be familiar with. (Some projects also use specific tools that should be separately documented.) As one broad principle to keep in mind: research coordinators in the lab typically stay 2-3 years, but many of our projects span a much longer period of time. We need everyone to perform protocols consistently, and to document their work carefully, so that the people who follow you can continue your work. 3.1 Slack Slack is the primary method of communication within the lab. You can customize your notification preferences (particularly important on the mobile app) so you are notified of important messages but also are not overwhelmed. Use the ‘@’ feature to get my attention if I need to get involved in a conversation, and make sure that you’re subscribed to the right channels (#general and #random for everyone, and as of this writing #agingcog, #code, #diversity, #dma, #gbd, and #neurotech depending on role/interests). Two important caveats about Slack: 1. Slack is not considered secure or HIPAA-compliant. No PHI (protected health information) belongs there, and in general any discussion involving individual research participants should be conducted via SECURE: e-mail rather than Slack. 2. Slack is not an archive. Our free lab account only allows us to view the last 10,000 messages, and to store 5gb of files–everything older than that disappears. So we use Slack for rapid, concurrent conversation–but anything that we might want to refer to later needs to be documented, most likely in GitHub. 3.2 Lab website Our lab website is where we communicate what we do and share our work with the outside world. This is hosted on our public GitHub repository, which you will need a GitHub account to edit. See the README at https://github.com/DecisionLabUCSF/decisionlabucsf.github.io to learn how to add your team profile. Because this is a public repository, anyone can read the code that we use to build the website, and it also has an open license inviting other investigators to use this code to design their own websites if they wish. (Kate Rankin’s lab is building a new website based on our website’s code…) As I say in the README, you can think of GitHub as like a supercharged Google Docs for code, allowing multiple people to work on code together, keeping track of who made what changes and when, making it easy to reverse changes that have been made, and allowing different versions to be developed at the same time. Tools like this are commonly used by software companies to maintain quality and avoid/fix bugs. We won’t be using all of the sophisticated tools in GitHub that they use, but generally I think scientists have a lot to learn from software developers: Facebook and Google can’t survive with sloppy code and neither can we. 3.3 Private Github repo In addition to our public repository, we also have a private repository that only lab members can view, which is where we document and edit our code, and maintain electronic lab notebooks. See the README at https://github.com/UCSFMemoryAndAging/decisionlab for details once your account is fully set up with permissions. A basic way of working with GitHub is just editing files on the GitHub website. However, to use GitHub for code that can be utilized by statistical/analytic packages like R and MATLAB, you will need to synchronize the lab repo to a folder on your computer (like a more technical version of Dropbox or Box). For more details on how to do this, see Chapter 9 of this lab handbook, RStudio analysis pathway. Here are some key initial points: 1. As with Slack, no protected data. PIDNs and other assigned identifiers can pop up infrequently, but we should avoid other participant-level data, and especially PHI and participant identifiers (names, dates of birth, addresses, demographics, etc.) 2. GitHub works best with text-editable files: code, markdown files, HTML, notebooks and the like. GitHub is much less useful for big files like images, Word and Powerpoint files, and they tend to clog things up for everyone else who is syncing to the repo. Images should be reduced in size whenever possible, and Microsoft Office files (Word, Powerpoint) should stay out of GitHub and go on the R: drive somewhere. 3. Please be consistent about naming and file organization, and be obsessive about documentation and comments! Again, other people will pick your work up after you, so please be kind to them in advance. 4. Try to keep line length to about 80 characters, and commit-pull-push! 3.4 R: drive (R:) To recap some points from above: Slack can’t be used for protected data or discussion of individual research participants, and isn’t an archive. GitHub is an archive for code and documentation, but also shouldn’t be used for protected data or identifiers (and documentation about individual participants should be limited), and shouldn’t be used for large files such as Microsoft Office documents. So: we use the R: drive for large datasets, especially those including personal identifiers, and other useful resources that are too large to put in GitHub. (Quick personal note: I’m often working from my laptop and home computer, on which I only mount the R: drive intermittently–so if you need me to look at something, it will often be a lot faster if it lives in GitHub or is temporarily available in Slack.) For instructions on how to mount the R: drive on your computer, see the Technology section of MACipedia. For using statistical packages, a model for how to utilize the R: drive with the GitHub repo is described in Section 9: RStudio Analysis Pathway. As a general overview, the pathway is: 1. Clone the decisionlab GitHub repo to your computer. 2. Save original dataset (e.g., a .csv or Excel file from Qualtrics or E-Prime) on the R: drive, and never write to this file again. 3. Clean the data, using a script and a logging file that are saved in GitHub, creating a cleaned data file that is saved back on the R: drive. 4. Analyze the data using other script and logging files that are saved in GitHub, generating new data files and graphics as necessary that are saved on the R: drive. Since everyone is using the R: drive for a variety of projects (which will continue on after you leave), keeping our group folder organized is a challenge! Please refer to Guide.docx on the R: drive which explains how the subfolders are organized. The subfolder R:includes a number of files that I hope you find useful, including the subfolder “Biostat_212” (lots of resources here on using Stata), the subfolders for “Papers” that include an archive of some classic papers and other ones that influence my thinking and our work together, and some other resources (fonts, scanned books) that I’ve posted in a perhaps questionably broad interpretation of their UCSF licenses… 3.5 LAVA and LAVAquery LAVA is the MAC’s primary database for all research-related patient and participant information, e.g. demographics, visits and scheduling, research diagnoses, specimens. It can only be accessed on a UCSF network, so if you need to access it remotely, you’ll need to sign in to the UCSF VPN through Pulse Secure (check out https://it.ucsf.edu/services/vpn for how to get the VPN set up). For our current projects, we use LAVA for the following: * Getting participants’ contact information to call/email them about enrolling in one of our studies * Enrolling participants under the ‘Enrollment’ and ‘Scheduling’ tabs once we have seen them for a study * Looking up participants’ most recent (and past) diagnoses for tracking enrollment numbers or running data analysis LAVAQuery is a (fairly) user-friendly querying tool that allows you to pull information from a selected subgroup of (or all) participants from the main LAVA dataset. There is both an older excel-based version and a newer web-based version. You can access these on the front page of the LAVA website. If you will be running any kind of analysis, you’ll most likely use LAVAquery to pull relevant participant information, download it as a csv file, and merge with your study data files. For training, visit the LAVA section of MACipedia and watch the training videos under the ‘My Lava’ tab on the website. 3.6 Deprecated/discouraged tools Excel. In general, Microsoft Excel is not an appropriate tool for data analysis or manipulations that are intended for presentation or publication. It can be used to explore datasets, but usually this is best done within a proper statistical/analytic software package such as R, Stata or Matlab. Excel does not preserve a record of changes that are made to a data file, so if errors are accidentally introduced they can be impossible to diagnose. Excel also has very poor tools for automation and replication: if transformations are made to a dataset and the dataset is later expanded with more observations, the same transformations must usually be applied manually to the new entries. We do use Excel sometimes to create or input data (e.g., for tracking participants) that is then imported to stats packages, but here again the potential for introduced errors is high so please proceed carefully! (Tools like REDCap and Access may be better for this.) Finally, some of the data tools we use (Qualtrics, LAVAquery) export csv and xls files–our general practice is to save them somewhere on the R: drive and avoid ever touching or rewriting to them directly, only importing from saved files into a proper stats package. Google Docs. In general, we have found that Google Docs is useful for sharing short-term projects, such as an abstract or IRB submission that needs to be edited by multiple people before submission. However, it has proven to be unwieldy for archiving/storage/documentation–we’ve noted problems with moving/deleting files, with governing permissions and file access, and in general with organization. So anything that we might want to refer back to in a matter of months or years should be appropriately organized in GitHub or the R: drive. Box. In theory, Box should be awesome. It’s supported by UCSF, can handle secure data like PHI, and can sync local copies to people’s individual computers–so this should be one tool that can do most of what we use GitHub and the R: Drive for. In practice, whenever we try to use Box for anything it becomes super-frustrating and never seems to work appropriately. So we don’t use it, but if anyone figures out how to use it effectively for shared work please let me know… "],
["project-management.html", "Chapter 4 Project Management", " Chapter 4 Project Management Our work centers around four primary projects: 1. Genes, Brains, and Decisions (GBD) 2. Neuroethics in Novel Neurotechnologies (Neurotech) 3. Decision-Making in Alzheimier’s Disease and Related Dementias (DMA) 4. Aging and Cognition Guidelines for managing each project can be found in the associated Standard Operating Procedure (SOP), each of which are located on the R-Drive: DMA: R:\\chiong\\DMA GBD: R:\\chiong\\GBD Neurotech: R:\\chiong\\Neurotech "],
["guidelines-for-clinical-interactions.html", "Chapter 5 Guidelines for clinical interactions", " Chapter 5 Guidelines for clinical interactions Originally from Ali Zahir and Madhu Manivannan Our work is centered around learning about and caring for older adults and patients with neurological disorders. As such, it is extremely important to us that we treat our research participants with respect, kindness, and understanding. After all, they are voluntarily devoting their time to help us, often with little to gain for themselves. These guidelines are by no means exhaustive, and are meant to highlight some considerations to take into account when interacting with our unique patient population. Please make sure to use person-centered language. (i.e. refer to our participants as patient with bvFTD as opposed to the “she’s a bvFTD”). In general, “person-with-x” is preferred to “x person”: i.e., “person with dementia” vs. “demented patient,” and similarly for terms such as “diabetic,” “epileptic,” etc. When greeting the patient: Introduce yourself, shake their hand, look them in the eyes, and treat them with the dignity they deserve. Acknowledge that they are providing us their time and energy by participating in our study. Our patients endure some rather grueling days and we want them to understand we appreciate them. Walk at their pace. At times, patients (or even healthy controls) can have a slow gait and speed and therefore you may need to slow your pace. In the room, allow the patient to be seated by the door. We don’t want them to feel trapped in our exam rooms. When obtaining consent from a patient, make sure to speak slowly and clearly. Avoid acronyms, esoteric terms, and jargon. Make the information in the consent form digestible for them. It helps to have a script prepared in your head that covers the following: Purpose of the study/what our lab is interested in What participation entails Risks and Benefits Rights as a participant Provide opportunity for questions Similarly, when explaining a task, please speak slowly. Pause frequently for questions and always check-in to see if the patient understands what you are saying. Always provide a patient enough space; do not dominate the interaction and allow them time to speak. Listen with intent. Make sure to listen to what the patient is saying, rather than focusing only on what you want the patient to do. Provide affirmations and reflect back to the patient what they have said so that they know you are listening. When interacting with a patient and their caregiver, be sure to remain neutral. You do not want to side with one party. If there is a disagreement between the caregiver and patient, do not interfere but make sure you are physically on the side of the patient. I’ve learned this allows the patient to not feel alienated. As you take patients through the task, be mindful of their capabilities and frustrations. If they appear agitated, uncomfortable, overly exhausted, or confused, pause the task and remind them that they can stop at any time. Be sensitive to their needs. Our tasks can be cognitively taxing, so it is important to continuously monitor the patient’s comfort level. Patients will sometimes directly tell you they do not want to complete the task. Assure them that this is perfectly fine, and thank them for their time and their willingness to come in for this research visit. Some patients will apologize for not understanding the task, asking lots of questions, or not being able to perform well on the task. Take the time to listen to their concerns, and sympathize (e.g. if a patient is frustrated at not being able to come up with words for a memory task, you might respond by acknowledging their difficulties and encouraging them on their performance). Don’t be condescending or patronizing–when in doubt, listen to what patients are saying. Sometimes that’s better than responding or trying to provide solutions. Always thank them for participating in our research–this is completely voluntary and our patients travel long distances and undergo hours of exhausting tasks and scans to help us with our research! "],
["hiring-and-onboarding.html", "Chapter 6 Hiring and Onboarding", " Chapter 6 Hiring and Onboarding Many of our protocols re: creating a ticket to post job openings, the hiring process, and onboarding can be found in the R drive, so I will just provide links to the appropriate folders/websites here. For lab managers/coordinators tasked with hiring and onboarding new hires: Hiring and Onboarding SOP located in the R-drive: R:. This provides an overview of: How to submit tickets to People Connect to create a job requisition for a new position How to get new hires set up with all their UCSF accounts, schedule orientations, and provide a general MAC overview Refer to MACipedia as needed For new hires: See the ‘Guide’ document (R:) for an overview of what’s in each folder on the R drive. Additional tools and training resources can be found in the R drive: R:. Check out ‘Training Guide’ document for a brief overview of the resources we use. Foundational/interesting articles for key topic areas can be found in the appropriate folders (see “Papers-aging,” “Papers-decision-making,” etc.) In-depth tutorials on Matlab, STATA, Python, etc. "],
["authorship.html", "Chapter 7 Authorship 7.1 Why it matters, and what it means 7.2 Tips for writing papers 7.3 More about figures 7.4 Notes on posters", " Chapter 7 Authorship One of the general aims of the lab (and often a personal aim for research coordinators and students) is to get members involved in authoring papers and posters. (Note: much of what follows primarily concerns authorship of papers. Because posters aren’t fully peer-reviewed, standards for authorship are less stringent and less contested.) 7.1 Why it matters, and what it means One source of ethical problems with authorship is that it serves several functions in science and academia, and these multiple roles can give rise to conflict and misunderstandings–both within research groups, and from one group to another. Among other things, we use authorship for: - Establishing credit for work. This is perhaps the key evidence used for promotions and faculty appointments, and is often a critical component of applications to graduate or professional school, residencies, postdocs, etc. - Proof of productive collaboration. This can be important, e.g., if our lab is planning to apply for a grant application with another lab. Having existing papers with authors from both groups would show a track record of working together. - Responsibility for the integrity of work. In some ways, this might be the most important yet most overlooked aspect of authorship. Authors are each responsible for ensuring that work is conducted rigorously and reported accurately. Assignment of authorship is relatedly important for detecting conflicts of interest in a piece of work. This can lead to several unfortunately common problems. Ghost authorship is when someone who authored or co-authored a paper is not listed as an author; e.g., if a pharma company scientist drafts a paper, which is then published under the name of a prominent academic. Courtesy authorship is when someone who didn’t author a paper is listed as an author; e.g., if someone adds their department chair as an author to curry favor with them. Relatedly, sometimes people will include an author’s name without permission, either due to sloppiness or in hopes that a paper will be reviewed more favorably if a prominent scientist is listed as an author. This is a problem because authors are all responsible for the integrity of a paper. Criteria for authorship vary across disciplines, and sometimes across journals in the same discipline. As a common starting point for discussion, we use the ICMJE guidelines applied by medical journals because they’re the most explicitly codified. In outline, the 4 criteria that must all be met for authorship are: - A. Intellectual contribution - B. Drafting or critically revising the paper - C. Approval of the final draft - D. Accountability (if the integrity of the work is challenged) Many problems in authorship come from discrepancies in the contributions that people make to (A) and (B). For example, someone who helps to initiate a project but then leaves might make major contributions to the design of the work, but if this person is not involved in drafting or revising the paper would not count as an author. Because of this, it’s very important that before writing begins on a paper, consideration should be given to (1) who has made significant intellectual contributions to be eligible for authorship, and (2) whether or how these people can be given a reasonable opportunity to contribute to drafting or revising the manuscript in order to count as authors. If you are planning to be 1st author on a paper (see below), please consult the MAC Authorship Guidelines when you are conceiving the project and as you are drafting the paper, to ensure that you’re not leaving out someone who should be eligible for authorship. 7.1.1 Authorship order The author positions that have established meanings across biomedical institutions are first, second, and last author. The first author is usually the person who initiated a project and did most of the work, including organizing the other authors and managing the submission process. The last author should be the mentor or supervisor, and is responsible for “big picture” issues–placing findings in context, targeting a journal, etc. The person who did the next most to complete the work is the second author, and pretty much everyone else is a “middle author.” Sometimes you’ll see two people listed as “co-first authors.” Since inevitably one person’s name does have to come before the other’s, credit doesn’t really get shared equally, so in most cases we should just make an active decision about who is first and who is second. 7.1.2 Involvement of RCs and students Involvement of more junior members of the research team as authors requires planning in advance, to ensure that they do make sufficient intellectual contributions and play a significant enough role in drafting the manuscript to count as authors. For example, while senior colleagues may plausibly contribute to (B) by drawing on their past scientific and publishing expertise to critically revise a completed draft, it is less plausible that junior colleagues are in a position to contribute in this way. Involvement of more junior colleagues requires planning at the outset of drafting. In my experience, the most feasible drafting roles for RCs and students include: writing parts of the Methods and Results, creating figures/tables, and organizing references. Reserving a paragraph or a figure for a junior colleague to contribute, even if it’s something the first author could have done, can be a way of ensuring that RCs or students who deserve credit for their intellectual contributions (e.g., of acquiring or analyzing data) also fulfill drafting criteria for authorship. If you are the an RC or student and are the first author of a paper, we will meet to discuss what this means for you in more detail. As a rough outline (see more below), papers for scientific/medical journals usually have the structure: - Introduction - Methods - Results - Discussion In my view, if I’m senior author I should expect to have more responsibility for the Introduction and Discussion, since these often involve putting our findings in a broader context. So while I expect you to have principal responsibility for the Methods, Results, and any figures or tables (including organizing the contributions of middle authors), I don’t expect too much from you in the Introduction and Discussion. It’s often a worthwhile exercise to have you take a first stab at drafting them, but in my experience I almost always rewrite them when RCs or students are first authors. So: don’t agonize too much over them–writer’s block over the Introduction and Discussion should never be the cause of delays in completing a manuscript draft. 7.2 Tips for writing papers Many people find writing intimidating. Some resources to get you started include: Strunk &amp; White, The Elements of Style. A classic of English composition, emphasizing clarity and brevity as services to the reader. Not every rule in here needs to be followed strictly, but if you understand the rules you’ll be in a better position to break them when needed. Warren Browner, Publishing and Presenting Clinical Research. More specific to medical/clinical research, walking you through the structure of papers by section (since much of this structure is shared with other scientific papers, this resource may be helpful even for non-clinical research). Mensh &amp; Kording, “Ten simple rules for structuring papers.” Addressed more to non-clinical scientific research, but again I think broadly helpful. PLoS Computational Biology 2017. In R:-methods-and-reliability. Gernsbacher, “Writing empirical articles.” Advances in Methods and Practices in Psychological Science, 2018. In R:-methods-and-reliability. 7.2.1 Choosing a journal This has gotten more complicated in recent years, in part because of the phenomenon of predatory journals. These are journals that often have names very similar to highly-respected journals, but that don’t actually exercise peer review or any real editorial oversight. These predatory journals often use a version of the “open-access” model–while legitimate open-access journals (like the PLoS journals) charge the authors a fee to cover the costs of editing so that readers can access papers for free, predatory open-access journals accept every paper sent to them so that they can collect fees from authors without actually providing editorial services. If you’re not already familiar with the journals in a given domain, it’s probably worthwhile to use the Journal Impact Factor as a rough guide to journals’ reputation. Impact factors are pretty flawed measures of journal quality (think of them like U.S. News college rankings), but they can give you a general impression of journals’ reputations and at least hopefully help distinguish predatory journals from similar-sounding legitimate journals. Another great tool is JANE (Journal/Author Name Estimator). This is a tool where you can input your title and/or abstract, and it will suggest journals (including rough impact factors) based on keyword and textual similarity. You can also use this tool to find articles and authors likely to be relevant to your work. Finally, in choosing a journal you should know that, at the time of this writing in 2019, the UC system does not have a contract with Elsevier, a major publishing company that publishes Neuron and other Cell Press journals, Cortex, the Current Opinion series, Lancet and Lancet Neurology, NeuroImage, Neuropsychologia, and many others. This means that UC students and faculty do not have library access to Elsevier journals, and many UC faculty and employees have decided not to write, review or edit for Elsevier journals until this is resolved. We have not adopted this position as lab policy, but it is something to keep in mind. 7.2.2 Title Most good scientific papers have a straightforward, simple finding–usually either of the form A &gt; B or A ~ B–which should similarly arise from a straightforward question. For your paper, you want your potential audience to quickly grasp your research question/finding so they can figure out whether it’s relevant to their interests. (And then hopefully read it!) It’s great if titles can include the Predictor, the Outcome, and the Population; e.g., Han and colleagues, “Financial literacy is associated with white matter integrity in old age.” Someone who is totally naive about this literature can still figure out the finding and probably can infer the research question. An expert in this literature can also figure out the motivation and relevance of the finding, and probably can also figure out much of the methods (a financial literacy questionnaire for the predictor, diffusion tensor imaging (DTI) for the outcome, in a healthy older population). 7.2.3 Introduction (present tense for what is currently known) If your finding takes the form A &gt; B or A ~ B, your introduction should show why it would matter to know that A &gt; B or A ~ B. Usually in 3-4 elegant paragraphs (except for psychology journals, those introductions are crazy long) you want to: - establish the scientific, clinical or public health importance of the topic - briefly summarize previous research in this area - identify shortcomings or gaps in current knowledge (focused on problems that your study will fix) - show how your study addresses these shortcomings or gaps (sketching an overview of your hypothesis, design, sample and methods) Basically, in the introduction you want to get your audience interested in the question that your study is set up to answer. 7.2.4 Methods (past tense for what you did) Now demonstrate to the reader how you plan to show that A &gt; B or A ~ B. Note that if the study has a preregistered protocol, it’s best to try to keep the description of the methods similar to the description in the protocol–when feasible, don’t even paraphrase but instead keep the exact same wording. Key overall elements of the Method are: - In clinical research, specifying the type of design (e.g., retrospective vs. prospective, case-control vs. cohort vs. RCT) - Research subjects: the population, the inclusion/exclusion criteria, the controls - Measurements, both for predictors and outcome variables. It’s important to organize these in a way that’s friendly to the reader. Usually it’s most straightforward to group the predictor variables together and then the outcome variables. Also see Browner for thoughts about how to convey the appropriate level of detail for your target audience. - Analysis: your statistical plan and why you used the tests that you used. Make sure you explain decisions you made that might seem odd to reviewers (e.g., excluding subjects and your rationale for doing so), and any missing data and how missing data were handled. 7.2.5 Results (past tense for what you found) A tricky thing for new scientific writers is understanding the relationship, on one hand, between the Methods and Results; and on the other hand, between the Results and the Discussion. - “All results must have methods; all methods must have results.” Basically, if you describe some procedure in your Methods, then whatever happened needs to show up in your Results. Similarly, if you describe some finding in your Results, your reviewers need to be able to figure out how you arrived at that finding from your Methods. - In general, the Results are supposed to describe what you found, and the Discussion is supposed to interpret them. So the Results are not supposed to include any interpretation–sometimes you can get away with something like, “As predicted, A &gt; B,” but some reviewers will balk even at that. Also, there should be no new results in the Discussion. It’s really important to structure your results so that your main or most interesting finding is obvious and easy to find. You don’t want it buried in the middle of a long paragraph or hugely complicated table or figure. The normal order is to start with a quick rundown of descriptive results (who your research participants were and what happened to them), and then get to your main analytic finding. Usually you want to keep your strongest findings first, and simple results (like main effects) before complex ones (interaction effects, multivariate models, validity checks); it’s also helpful when possible to mirror the ordering of your Methods. One especially tricky thing is figuring out which results just to describe in the main text, as opposed to creating a figure or table. See below for more on figures. Usually if your result is just a matter of one or two numbers (e.g., your finding is A &gt; B and you want to show A and B’s point estimates and confidence intervals), then you should just put this in text. Overall, the text and the tables/figures should complement one another. As Browner (Chapter 5) notes, new investigators often write the text around their tables and figures, e.g., “Table 1 shows… The multivariate results can be seen in Table 2…” See Browner for suggestions on how to use the text to help your reader interpret the data in your figures and tables. 7.2.6 Discussion (past tense for talking about what you found, present tense for what it means) It’s helpful to go back to the question that you posed in the Introduction, and then to indicate that your study has answered this question. Synthesize and summarize the key findings, including particular mention of any that are surprising or otherwise noteworthy. Try to use words rather than numbers to do this; if numbers are included they should be kept simple. Then write about what you think the results mean, indicating how strongly you believe them. Some people treat the Discussion as a “sales job”–I don’t think this is a good way to view your relationship to the reader. My preference is to try to perform a service to the reader, by helping the reader to place your findings in appropriate context. So if you think your findings really do “demonstrate” that A &gt; B or A ~ B, go ahead and say so; but if you know that the data admit of alternative interpretations, you can signal this by saying that your findings “suggest” that A &gt; B, or that a reasonable interpretation is that A &gt; B. If there are special features of your study you might choose to highlight them, but avoid bragging or claims of priority (“this is the first/largest…”). Briefly synthesize previous research to show how your work relates to the existing literature. Set up your limitations and explain them (e.g., why you used the sample you did, or why a given statistical model didn’t fit), including not only design/methodological limitations, but also limitations of your interpretation of findings. 7.3 More about figures A great guide for thinking about how to communicate statistics with figures is Tufte’s The Visual Display of Quantitative Information (I have a copy you can borrow if you like). Tufte’s focus is on how we can use design to help facilitate understanding of relationships in data. Another resource is Rougier, Droettboom &amp; Bourne, “Ten simple rules for better figures.” PLoS Computational Biology 2014. (In R:-methods-and-reliability) raster-vector One important thing to understand about figures is the difference between vector and raster graphics. Raster graphics are the most familiar to people–these are the images created by digital cameras, and are composed of a grid of tiny squares called pixels (similar to needlepoint). To edit a raster image, we use programs like Paint or Photoshop to change the colors assigned to each pixel. Vector graphics are instead made up of lines and curves that are mathematically defined. (If you’ve used the drawing tools in Powerpoint or Google Slides, these are vector graphics.) To edit a vector image, we use programs like Illustrator or Affinity Designer to move or change the points that define the lines and curves. Because raster graphics are actually made up of tiny squares, there is a limit to how far you can magnify them–zoom in too close, and you get jagged effects where what is supposed to be a curve or slanted line turns out actually to be blocky. Meanwhile, vector graphics are defined mathematically, so you can zoom in on them indefinitely. Related to this, it’s very easy to convert vector graphics to raster graphics, but it’s hard to go in the other direction. Most figures, like line or bar graphs created by Stata or R, are produced as vector graphics. These should be saved in an appropriate vector format (most commonly EPS, sometimes AI), and edited as vector graphics. If you convert them to raster, then it will be nearly impossible to edit them again as vector graphics. Some other figures (such as from neuroimaging) are generated as raster graphics. When editing these, please be aware that the screen resolution of your monitor (often something like 72 dots per inch) is much lower than print resolution (300-600 dpi). If your graphics are intended for publication, they need to be created and edited in print resolution. 7.3.1 Style and design As emphasized by Rougier and colleagues, do not just use the defaults from the stats program (Stata, R, Matlab…) used to create your figures. Instead, you want your figures to demonstrate your active thinking about the most revealing and helpful ways to display your data–e.g., if there are multiple colors, this should be because the colors denote something meaningful or informative that you’re signaling your reader to pay attention to, and not just because the colors are part of the default settings on your stats package. You have two main options for customizing your figures. First, you can modify output settings of your stats program, and in this way tweak your labels, legends, line thickness and color, point markers and so forth. Second, you can save the figure in an editable vector format such as EPS, and then open the image file in a vector editing program such as Illustrator or Affinity Designer. In practice, I usually rely on a combination of the two approaches, as some changes are easier and more efficient to do in a stats program, and others are easier to do in an editing program. For consistency, I recommend changing all figure labels to Helvetica, unless another font is specified by the journal you’re submitting to. (Stylistic consistency will help when we want to combine figures from different projects, e.g. in future grant applications or presentations. In general, our lab products, following UCSF style, use the fonts Helvetica and Garamond. In a pinch you can use Arial in place of Helvetica, but I will complain about it.) See the website-and-identity/fonts folder in the R: drive. Finally, when possible it’s nice to use the UCSF color scheme. Just using any consistent color scheme will help later on when we want to combine figures from different projects, and relying on the UCSF scheme will help things look consistent on posters. To access the UCSF color scheme, go to the UCSF brand guide for colors. UCSF colors You’ll notice that each color has numbers associated for RGB (quantities of red, green and blue light). To use these in your program, you need to go beyond the preset colors–look for a color palette or an option for More Colors… Then from that find the option to enter the numeric RGB values directly (rather than trying to choose a shade that matches), and use the values from the UCSF color scheme: more colors 7.4 Notes on posters I like to think of posters as preparation for writing the paper that will eventually result from your project. Selfishly, in the poster session you’ll want to talk to people who know enough about your topic or methods to be helpful in your thinking about how to write up the project–or, maybe if you’re lucky, to PIs or other people whom you’re trying to impress. Communicating your findings to other people who are not in the area is rewarding but of secondary importance. Trying to view posters in poster sessions is cognitively overwhelming. Some conferences like SfN have thousands of posters. Personally, I’ve found that I can only take in a handful of posters in a session, and I actively ignore everything else in the poster hall because otherwise details get jumbled together and I don’t remember anything. When you’re designing your poster, think about ways to specifically draw in the people you want to talk to. Depending on what stage you’re at in your project, you might be looking particularly for feedback on your framework, or methods, or analysis, or findings. Whatever it is, you want it to be obvious from a distance to people who are interested in that area that your poster is relevant to them. E.g., use eye-catching figures, but only if those figures quickly communicate what your project is about, and particularly the parts that you’re looking for feedback about. And knowing that people are cognitively overtaxed, don’t make them work to figure out what you mean: use labels and headings to help people know what they’re looking at and what you want them to think. mock poster Here’s a mockup of a poster (not the actual one–don’t examine too closely) that demonstrates some strategies for organizing a poster: - High-value material is placed at the top of each panel: on the left, the conceptual framework and motivation; in the middle, the major figure; and on the right, the takeaways. This makes these elements easier to spot from a distance in a crowded hall. - Finer details are placed at the bottom: these are things you’re including for thoroughness and might be of interest to someone who’s already interested in the poster, but aren’t things that you expect to draw people in. Here I’ve put background info on the study from which the data is derived, a table characterizing the participants (usually “Table 1,” but here crammed at the bottom and in tiny font), and Acknowledgments/References. - The poster is organized to make it easy for people to visually scan and find the parts they’re interested in. There’s a fair amount of white space, so the text isn’t all crowded on top of itself. (Even though this required us to make the font size a little smaller than it might otherwise have been.) The headers use a consistent font/color convention and a familiar Intro-Methods-Results-Discussion format. - When someone is reading a paper, it’s the only thing they’re looking at, so you don’t always have to spell everything out for them. When someone is looking at a poster, it’s one thing among many others in a crowded and distracting poster hall, so you have to make it very obvious what lessons they’re supposed to draw or they’ll get frustrated and move on. Here, each figure in the Results section has a descriptive title, and also includes brief text to help viewers know what’s going on. Similarly with the takeaways: try to come up with two or three really quick and clear messages that viewers should walk away with at the end. - Note that the graphics use the UCSF color scheme (as explained in More about figures - Style and design above), so they’re easy to combine from different sources without clashing. (The “Care Ecosystem study” diagram is taken directly from Kate Possin’s paper about the Care Ecosystem.) One last picky thing. The Windows versions of Microsoft Word and Powerpoint have an annoying issue where, if you use “Save As…” with the “Save as type: PDF” option, they convert all instances of Helvetica to Arial. To get around this, use the “Print…” menu and choose “Microsoft Print to PDF” (or similar options from Foxit or Adobe if you have them installed) as the printer. (Not an issue on Macs.) "],
["reliability-and-open-science.html", "Chapter 8 Reliability and Open Science 8.1 Approaches to promote reliability 8.2 Open science", " Chapter 8 Reliability and Open Science Sadly, it has become clear that many published scientific studies fail to replicate–that is, other scientists are unable to produce the same results, such that reported findings cannot be relied upon by others (and, therefore, that these “findings” do not represent a genuine contribution to human knowledge). While this problem partly reflects inherent challenges in scientific discovery, repeated and pervasive failures of reproducibility call the value of science into question (see Section 1: Organizing Principles). Lack of reproducibility has many sources, which we should understand so as to minimize their influence in our work (and also to avoid being misled when reading papers by other labs): 1. Dependence on unreliable methods. This includes “p-hacking” (abuse of excessive researcher degrees of freedom in study methods) “HARKing” (hypothesizing after results are known), and widespread misunderstandings of statistical approaches appropriate to exploratory as opposed to confirmatory analyses; also, use of underpowered study designs and related confusions about the difference between power and positive predictive value. For more backfground see Simmons, Nelson &amp; Simonsohn 2011, Button et al 2013, and Ioannidis 2005 in the Resources-methods-and-reliability folder of the R: drive. 2. Human error. Every time someone touches or manipulates a dataset, this intoduces the possibility of unintended errors. We expect human beings to make errors, so we have to be vigilant about checking our own work and asking other members of our team to help check our work. This means that we have to use methods that make it possible to identify and diagnose errors, such as by tracking and logging transformations of data. This is one reason that we strongly prefer that manipulations be performed in code, rather than manually in programs like Excel (see again “Deprecated/discouraged tools” in Section 3: Lab Resources Overview). We also always want to preserve and archive our raw, unmanipulated datasets so that we can always start over if absolutely necessary. 3. “Bad” luck. Suppose that 20 different research teams are investigating the same false hypothesis. Even if the null hypothesis holds (there is no true finding), given a conventional significance value of 0.05 it is likely that one of these teams will reach a statistically significant result by chance. Given a publishing bias for positive over negative studies (see point 4 below), this false finding is likely to be published while the 19 correct, negative studies would be ignored. This is often a hollow accomplishment, as labs tend to base future studies on previous findings, so publishing a paper based on a false chance finding can lead to years of wasted work pursuing scientific dead ends. 4. Perverse incentives in science. Researchers face tremendous career pressures to publish in order to keep their jobs or secure funding, and publishers generally favor positive over negative results and “surprising” findings over those that follow more directly from what is already known. All of these incentives promote the production and dissemination of false positive findings. Finally, there’s one more sense of “reproducibility” that is important for the organization of our lab. As emphasized earlier, most members of the lab only stay for 2-3 years, so much of the work you’re doing now will be completed by future members. You should try to document your work in a way that it will be intelligible to future lab members, who can then pick up your work without needing to rebuild your analyses from scratch. 8.1 Approaches to promote reliability There are some broad principles and practices that we use in the lab to promote the reliability of our work, that everyone (particularly those engaged in quantitative research) should understand: 1. Distinguishing exploratory vs. hypothesis-driven design. One of the major problems in quantitative research is that many scientists’ aims are in fact exploratory (e.g., trying to look for new things and elucidate processes that haven’t yet been discovered), but our accepted statistical thresholds for publication (e.g., p values and confidence intervals) implicitly assume confirmatory (hypothesis-driven) methods. So many scientists, often without really understanding the statistical issues involved, report exploratory work as if it were hypothesis-driven (e.g., reporting on p values) even though the hypotheses being tested were not actually formulated prior to looking at the data. This violates the assumptions of significance testing and so often contributes to false positive reports. Unfortunately, how to properly report exploratory research remains an open question that the scientific community is still working out. Still, we should remain careful in our thinking about when our aims are exploratory and when they’re hypothesis-driven, and we should not try to pass off exploratory work as hypothesis-driven. 2. Study pre-registration when feasible; in all cases, pre-planning. In general, if we are conducting a hypothesis-driven study, it makes good sense to pre-register our study design. Generally we use the Open Science Framework. Pre-registration has been standard practice in randomized controlled trials for many years, and is now being extended to other kinds of experimental research. Even when we are not pre-registering studies or we are planning purely exploratory work, it is generally a useful exercise to think in advance about what questions we are interested in, and what would be the best approaches to answer these questions, before we start poking around in datasets. Hopefully this can help us avoid treating noise in our dataset as signal. 3. Documentation. This is something I struggle with, but is crucially important for other members of the lab and also for yourself. It’s natural to assume, when you’re totally cognitive engaged in a problem, that it will be very easy for you to recall later what you did and why. In fact we all tend to overestimate our recall for these details, once we move on to solve new problems. It’s really important to use our electronic lab notebooks in the private Github repo to document our thought processes. (This will help you in the future as well.) This is particularly crucial for recording a paper trail of analytic decisions, data cleaning, and other decisions affecting findings (e.g., leaving out outliers and why). 4. “Literate programming.” This refers to a different way of thinking about work that we do in code. As expressed by the computer scientist Donald Knuth: &gt; &quot;Let us change our traditional attitude to the construction of programs. Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to humans what we want the computer to do.\" In other words, think of your code (whether in R, Stata, Matlab, or any other language) as like an essay intended to communicate to other human beings. This should follow the order and logic of human thought, and not just the forms imposed by the computer. It is not only important that the code “works” (in that it makes the computer perform the operations that you intend); it’s also important that people who read your scripts (including your future self) can understand the flow of your thinking and what each piece of code is intended to do. Section 9: RStudio Analysis Pathway provides a detailed tutorial to one implementation of literate programming. Coming soon: code review. As above, human error is common and expected. We should all be diligent about checking our own work, but in most cases we also need other people to help us find errors that we can’t find ourselves. Software developers don’t rely solely on individual programmers to find errors in their code, but instead use collaborative tools so that teams can check one another. This is one of the reasons why all of our code is intended to be shared in the Github repository, rather than just being distributed across different people’s desktop computers. In the coming year, I’m hoping to start using collaborative tools in Github that facilitate review of our code by other lab members. This is another reason why it’s important to write “literate code” that can be readily understood by others, and not just a series of computational operations. Also coming soon: publishing our code. Increasingly, it will be an expectation that when someone publishes a scientific study, that they also make their datasets and code publicly accessible so that other scientists can check their work. After all, if you’re not confident enough in your code to allow other scientists to review it, then should you be confident enough to publish findings based on this code? If you’re reading a study but the authors are not willing to publish their code, should you believe what they report? We will start producing our code in a manner that facilitates such sharing, as described further in Section 9: RStudio Analysis Pathway. 8.2 Open science The term “open science” is used in many different ways by different scientists, but in general the ideas above follow many of the broad principles of open science. We want to produce work that can be relied upon, used, and extended by other scientists. When feasible, we will pre-register studies, particularly for hypothesis-driven work. We will also aim to post our code when we publish our studies, in part to encourage ourselves to check our code carefully. When permitted by publishers’ policies, we will post free versions of our papers on our lab website so that scholars everywhere can read them. One respect in which our science may not be fully open has to do with the openness of our datasets. Much of our work involves stigmatized disorders such as dementia, or potentially sensitive topics such as money management or vulnerability to financial errors in aging. For published studies, we should review the consent forms for particular studies as well as MAC data sharing policies to see what forms of data sharing are consistent with center policies and participants’ reasonable expectations regarding the privacy of potentially sensitive data. When feasible and ethically permissible, we should aim to share our datasets (potentially also in redacted form). "]
]
